{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbJorS9tKbJK"
   },
   "source": [
    "# Data Crawling\n",
    "\n",
    "Crawling of PDF and retrieving the data from \n",
    "https://www.supremecourt.gov.sg/news/supreme-court-judgments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyb-VryKrUiy",
    "outputId": "f2cf2302-0dcf-4a93-db9a-8af1ba1c2ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping selenium as it is not installed.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting selenium==3.3.0\n",
      "  Downloading selenium-3.3.0-py2.py3-none-any.whl (925 kB)\n",
      "\u001b[K     |████████████████████████████████| 925 kB 20.6 MB/s \n",
      "\u001b[?25hInstalling collected packages: selenium\n",
      "Successfully installed selenium-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall selenium \n",
    "!pip install -U selenium==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ccbMZWV8vb1S"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qygtpLq0PLr0"
   },
   "outputs": [],
   "source": [
    "drive_path = '/note'\n",
    "normal_url = 'https://en.wikipedia.org/w/index.php?title=Special:Search&limit=1000&offset=0&ns0=1&search=United'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/crucy.1506241137.v3.23/countries/tmp/\"\n",
    "\n",
    "html_content = requests.get(url).text\n",
    "# Parse the html content\n",
    "soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "\n",
    "table_content = soup.find(\"table\")\n",
    "\n",
    "url_arr = []\n",
    "for link in  table_content.find_all(\"a\", href=True):\n",
    "\n",
    "    url_link = \"https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/crucy.1506241137.v3.23/countries/tmp/{url}\".format(url=link.text)\n",
    "    url_arr.append(url_link)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actaeon_Group\n",
      "Afghanistan\n",
      "Albania\n",
      "Aldabra_Isl\n",
      "Aleutians\n",
      "Algeria\n",
      "Amsterdam_Isl\n",
      "Andaman_Isl\n",
      "Andorra\n",
      "Angola\n",
      "Anguilla\n",
      "Antigua\n",
      "Antipodes_Isl\n",
      "Argentina\n",
      "Armenia\n",
      "Ascension\n",
      "Auckland_Isl\n",
      "Australia\n",
      "Austria\n",
      "Azerbaijan\n",
      "Azores\n",
      "Bahamas\n",
      "Bahrain\n",
      "Banaba\n",
      "Bangladesh\n",
      "Barbados\n",
      "Barbuda\n",
      "Bassas_da_India\n",
      "Belarus\n",
      "Belgium\n",
      "Belize\n",
      "Benin\n",
      "Bermuda\n",
      "Bhutan\n",
      "Bioko\n",
      "Bolivia\n",
      "Bonin_Isl\n",
      "Bosnia-Herzegovinia\n",
      "Botswana\n",
      "Brazil\n",
      "Brunei\n",
      "Bulgaria\n",
      "Burkina_Faso\n",
      "Burundi\n",
      "Cabinda\n",
      "Cambodia\n",
      "Cameroon\n",
      "Campbell_Isl\n",
      "Canada\n",
      "Canary_Isl\n",
      "Cape_Verde_Isl\n",
      "Caroline_Isl\n",
      "Central_African_Rep\n",
      "Chad\n",
      "Chagos_Archipelago\n",
      "Chile\n",
      "China\n",
      "Christmas_Isl\n",
      "Cocos_Isl\n",
      "Colombia\n",
      "Comoros\n",
      "Congo\n",
      "Cook_Isl\n",
      "Costa_Rica\n",
      "Croatia\n",
      "Crozet_Isl\n",
      "Cuba\n",
      "Curacao_Isl\n",
      "Cyprus\n",
      "Czech_Republic\n",
      "Denmark\n",
      "Djibouti\n",
      "Dominican_Republic\n",
      "Dominica\n",
      "Ducie_Isl\n",
      "Easter_Isl\n",
      "Ecuador\n",
      "Egypt\n",
      "El_Salvador\n",
      "Equatorial_Guinea\n",
      "Eritrea\n",
      "Estonia\n",
      "Ethiopia\n",
      "Faeroes\n",
      "Falkland_Isl\n",
      "Fernando_de_Noronha\n",
      "Fiji\n",
      "Finland\n",
      "France\n",
      "Franz_Joseph_Land\n",
      "French_Guiana\n",
      "Gabon\n",
      "Galapagos_Isl\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Gibraltar\n",
      "Gough_Isl\n",
      "Grand_Cayman\n",
      "Greece\n",
      "Greenland\n",
      "Grenada\n",
      "Guadalupe\n",
      "Guadeloupe\n",
      "Guatemala\n",
      "Guinea-Bissau\n",
      "Guinea\n",
      "Guyana\n",
      "Haiti\n",
      "Hawaii\n",
      "Heard_Isl\n",
      "Henderson_Isl\n",
      "Honduras\n",
      "Hong_Kong\n",
      "Hungary\n",
      "Iceland\n",
      "India\n",
      "Indonesia\n",
      "Iran\n",
      "Iraq\n",
      "Ireland\n",
      "Isl_da_Trindade\n",
      "Isl_de_Horn\n",
      "Isl_de_la_Bahia\n",
      "Isl_de_Providencia\n",
      "Isl_de_San_Andres\n",
      "Isl_Glorieuses\n",
      "Isl_Wallis\n",
      "Israel\n",
      "Italy\n",
      "Ivory_Coast\n",
      "Jamaica\n",
      "Jan_Mayen\n",
      "Japan\n",
      "Jordan\n",
      "Juan_Fernandez_Isl\n",
      "Kara_Sea_Isl\n",
      "Kazakhstan\n",
      "Kenya\n",
      "Kerguelen_Isl\n",
      "Kiribati\n",
      "Komandorskiye_Isl\n",
      "Kuril_Isl\n",
      "Kuwait\n",
      "Kyrgyzstan\n",
      "Laccadive_Isl\n",
      "Laos\n",
      "La_Tortuga_Isl\n",
      "Latvia\n",
      "Lau_Group\n",
      "Lebanon\n",
      "Lesotho\n",
      "Liberia\n",
      "Libya\n",
      "Liechtenstein\n",
      "Line_Isl\n",
      "Lithuania\n",
      "Lord_Howe_Isl\n",
      "Luxembourg\n",
      "Macau\n",
      "Macedonia\n",
      "Macquarie_Isl\n",
      "Madagascar\n",
      "Madeira\n",
      "Malawi\n",
      "Malaysia\n",
      "Maldives\n",
      "Mali\n",
      "Malta\n",
      "Marianas\n",
      "Marquesas\n",
      "Marshall_Isl\n",
      "Martinique\n",
      "Mauritania\n",
      "Mauritius\n",
      "Mexico\n",
      "Moldavia\n",
      "Monaco\n",
      "Mongolia\n",
      "Montserrat\n",
      "Morocco\n",
      "Mozambique\n",
      "Myanmar\n",
      "Namibia\n",
      "Nauru\n",
      "Nepal\n",
      "Netherlands\n",
      "New_Caledonia\n",
      "New_Siberian_Isl\n",
      "New_Zealand\n",
      "Nicaragua\n",
      "Nicobar_Isl\n",
      "Nigeria\n",
      "Niger\n",
      "Niue\n",
      "Norfolk_Isl\n",
      "Northern_Marianas\n",
      "North_Korea\n",
      "Norway\n",
      "Novaya_Zemlya\n",
      "Oman\n",
      "Pakistan\n",
      "Palau_Isl\n",
      "Panama\n",
      "Papua_New_Guinea\n",
      "Paracel_Isl\n",
      "Paraguay\n",
      "Peru\n",
      "Philippines\n",
      "Phoenix_Isl\n",
      "Poland\n",
      "Portugal\n",
      "Prince_Edward_Isl\n",
      "Puerto_Rica\n",
      "Qatar\n",
      "Reunion\n",
      "Rodrigues_Isl\n",
      "Romania\n",
      "Russia\n",
      "Rwanda\n",
      "Ryukyu_Isl\n",
      "Samoa\n",
      "San_Marino\n",
      "Sao_Tome_+_Principe\n",
      "Saudi_Arabia\n",
      "Senegal\n",
      "Severnaya_Zemlya\n",
      "Seychelles\n",
      "Sierra_Leone\n",
      "Singapore\n",
      "Slovakia\n",
      "Slovenia\n",
      "Society_Isl\n",
      "Socotra\n",
      "Solomon_Isl\n",
      "Somalia\n",
      "South_Africa\n",
      "South_Georgia\n",
      "South_Korea\n",
      "Spain\n",
      "Sri_Lanka\n",
      "St_Croix\n",
      "St_Helena\n",
      "St_Kitts\n",
      "St_Lucia\n",
      "St_Vincent\n",
      "Sudan\n",
      "Surinam\n",
      "Svalbard\n",
      "Swan_Isl\n",
      "Swaziland\n",
      "Sweden\n",
      "Switzerland\n",
      "Syria\n",
      "Tajikistan\n",
      "Tanzania\n",
      "Thailand\n",
      "Togo\n",
      "Tokelau_Isl\n",
      "Tonga\n",
      "Trinidad_and_Tobago\n",
      "Tristan_da_Cunha\n",
      "Tromelin_Isl\n",
      "Truk_Isl\n",
      "Tuamotu\n",
      "Tubuai_Isl\n",
      "Tunisia\n",
      "Turkey\n",
      "Turkmenistan\n",
      "Tuvalu\n",
      "Uganda\n",
      "Ukraine\n",
      "United_Arab_Emirates\n",
      "United_Kingdom\n",
      "Uruguay\n",
      "USA\n",
      "Uzbekistan\n",
      "Vanatu\n",
      "Venezuela\n",
      "Vietnam\n",
      "Virgin_Isl\n",
      "Western_Sahara\n",
      "Wrangel_Isl\n",
      "Yemen\n",
      "Yugoslavia\n",
      "Zaire\n",
      "Zambia\n",
      "Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "for link in url_arr:\n",
    "    html_content = requests.get(link).text\n",
    "    name = link.replace(\"https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/crucy.1506241137.v3.23/countries/tmp/crucy.v3.23.1901.2014.\", \"\")\n",
    "    name = name.replace(\".tmp.per\",\"\")\n",
    "    print(name)\n",
    "    with open(\"data/{url}.txt\".format(url=name), 'w') as f:\n",
    "            f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "countries = open(\"countries.txt\", \"r\")\n",
    "countries_list = []\n",
    "                 \n",
    "lines = file1.readlines()\n",
    "for line in Lines:\n",
    "    countries_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "vMEt706CjeEL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dirlist = os.listdir(\"data\")\n",
    "\n",
    "years = {}\n",
    "\n",
    "for files in dirlist:\n",
    "# # Using readlines()\n",
    "    file1 = open(\"data/\" + files, 'r')\n",
    "    lines = file1.readlines()\n",
    "  \n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in lines:\n",
    "        if count < 98: \n",
    "            count = count + 1\n",
    "            continue\n",
    "        if count ==101:\n",
    "            break\n",
    "        temp = line.split(\" \")\n",
    "        result = [n.replace(\"\\n\",\"\") for n in temp if n.strip() != \"\"]\n",
    "\n",
    "        \n",
    "        country = {}\n",
    "\n",
    "        country_name = files.replace(\".txt\", \"\")\n",
    "        country_name = country_name.replace(\"_\", \" \")\n",
    "       \n",
    "        total_temp = 0.0\n",
    "        \n",
    "        temp={}\n",
    "        for n in range(12):\n",
    "            total_temp = total_temp + float(result[n + 1])\n",
    "            temp[n + 1] = float(result[n + 1])\n",
    "        temp[\"average\"] = total_temp/12\n",
    "        country[country_name] = temp\n",
    "        \n",
    "  \n",
    "        if result[0] not in years:\n",
    "            years[result[0]] = []\n",
    "            \n",
    "        years[result[0]].append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {}\n",
    "for y in years :\n",
    "    c = {}\n",
    "    z = 0\n",
    "    for country_dict in years[y]:\n",
    "\n",
    "        z = z + 1\n",
    "   \n",
    "        for key, value in country_dict.items():\n",
    "            c[key] = value\n",
    "\n",
    "    p[y] = c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('temp_change.json', 'w') as outfile:\n",
    "    json.dump(p, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytnZAReWhyao"
   },
   "outputs": [],
   "source": [
    "# # def crawl_label():\n",
    "# csv_columns = ['url']\n",
    "# keywords = [\"Computer\", \"Glasgow\", \"United\", \"Kingdom\", \"Library\", \"Fog\", \"Empires\", \"Doctor\", \"Hospital\", \"Bachelor\",\n",
    "#             \"Degree\", \"Internet\", \"Things\", \"Information\", \"Info\", \"Retrieval\", \"Retrieve\", \"Universe\", \"University\"]\n",
    "# keywords = [ \"Empires\"]\n",
    "\n",
    "# for key in keywords:\n",
    "#   csv_list = []\n",
    "\n",
    "#   url = \"https://en.wikipedia.org/w/index.php?title=Special:Search&limit=500&offset=0&ns0=1&search={title}\".format(title=key)\n",
    "#   print(url)\n",
    "#   headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'}\n",
    "\n",
    "#   html_content = requests.get(url, headers = headers).text\n",
    "#   # Parse the html content\n",
    "#   soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "#   # Find the block\n",
    "#   content_text = soup.find(id=\"mw-content-text\")\n",
    "#   searchresults = content_text.find(\"div\", {\"class\": \"searchresults\"}) \n",
    "#   mw_search_results = searchresults.find(\"ul\", {\"class\": \"mw-search-results\"}) \n",
    "\n",
    "#   for link in  mw_search_results.find_all(\"a\", href=True):\n",
    "#     csv_list.append(\"https://en.wikipedia.org{url}\".format(url=link[\"href\"]))\n",
    "\n",
    "#   with open(\"{url}.txt\".format(url=key), 'w') as f:\n",
    "#       for line in csv_list:\n",
    "#           f.write(line)\n",
    "#           f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvtM6v_DNDTW"
   },
   "source": [
    "# Step 2) Saving the Label into CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htCv1x7eN62c"
   },
   "source": [
    "### 1) Save File as CSV Function\n",
    "\n",
    "**<font color='red'>Retrieve all the necessary labels and data and saved it in the form of a csv</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4WgSjVbLv41"
   },
   "outputs": [],
   "source": [
    "def save_csv(dict_data, filename, folder_path, csv_columns):  \n",
    "  # Check if the file exists\n",
    "  if not os.path.isdir(folder_path):\n",
    "    print(\"File create\")\n",
    "    os.mkdir(folder_path)\n",
    "    \n",
    "  try:\n",
    "      with open(folder_path + filename, 'w') as csvfile:\n",
    "          writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "          writer.writeheader()\n",
    "          for data in dict_data:\n",
    "              writer.writerow(data)\n",
    "  except IOError:\n",
    "      print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xubdPJ9gOIHE"
   },
   "source": [
    "### 2) Crawling of Label\n",
    "**<font color='red'>Retrieve and pass the content in the an array of a dictionary</font>**\n",
    "\n",
    "Similarly to Crawling of PDF, however we are inspecting the elements in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YzcOfaoJCoh",
    "outputId": "11c22ad5-a092-4e2a-eff9-0f7e913a8efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/w/index.php?title=Special:Search&limit=500&offset=0&ns0=1&search=Empires\n"
     ]
    }
   ],
   "source": [
    "# # def crawl_label():\n",
    "# csv_columns = ['url']\n",
    "# keywords = [\"Computer\", \"Glasgow\", \"United\", \"Kingdom\", \"Library\", \"Fog\", \"Empires\", \"Doctor\", \"Hospital\", \"Bachelor\",\n",
    "#             \"Degree\", \"Internet\", \"Things\", \"Information\", \"Info\", \"Retrieval\", \"Retrieve\", \"Universe\", \"University\"]\n",
    "# keywords = [ \"Empires\"]\n",
    "\n",
    "# for key in keywords:\n",
    "#   csv_list = []\n",
    "\n",
    "#   url = \"https://en.wikipedia.org/w/index.php?title=Special:Search&limit=500&offset=0&ns0=1&search={title}\".format(title=key)\n",
    "#   print(url)\n",
    "#   headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'}\n",
    "\n",
    "#   html_content = requests.get(url, headers = headers).text\n",
    "#   # Parse the html content\n",
    "#   soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "#   # Find the block\n",
    "#   content_text = soup.find(id=\"mw-content-text\")\n",
    "#   searchresults = content_text.find(\"div\", {\"class\": \"searchresults\"}) \n",
    "#   mw_search_results = searchresults.find(\"ul\", {\"class\": \"mw-search-results\"}) \n",
    "\n",
    "#   for link in  mw_search_results.find_all(\"a\", href=True):\n",
    "#     csv_list.append(\"https://en.wikipedia.org{url}\".format(url=link[\"href\"]))\n",
    "\n",
    "#   with open(\"{url}.txt\".format(url=key), 'w') as f:\n",
    "#       for line in csv_list:\n",
    "#           f.write(line)\n",
    "#           f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSC3010_Wiki_Crawler",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
